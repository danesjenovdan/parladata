import _pickle as pickle
import pycrfsuite
from tagger.train_lemmatiser import extract_features_lemma
from tagger.train_tagger import extract_features_msd
# from train_lemmatiser import extract_features_lemma
# from train_tagger import extract_features_msd

class Lemmatiser:
    """
    Wraper class for the tagger
    Use tag_and_lemmatise_tokens()
    """

    trie = pickle.load(open('tagger/sl.marisa', 'rb'), encoding='bytes')
    tagger = pycrfsuite.Tagger()
    tagger.open('tagger/sl.msd.model')
    lemmatiser = {'model': pickle.load(open('tagger/sl.lexicon.guesser', 'rb'), encoding="bytes"),
                  'lexicon': pickle.load(open('tagger/sl.lexicon', 'rb'), encoding="bytes")}

    def __init__(self):
        print('Tagger initialised.')

    def tag_sent(self, sent):
        return self.tagger.tag(extract_features_msd(sent, self.trie))

    def tag_lemmatise_sent(self, sent):
        return [(a, self.get_lemma(b, a)) for a, b in zip(self.tag_sent(sent), sent)]

    def get_lemma(self, token, msd):
        lexicon = self.lemmatiser['lexicon']
        key = token.lower() + '_' + msd
        if key in lexicon:
            return lexicon[key][0].decode('utf8')
        if msd[:2] != 'Np':
            for i in range(len(msd) - 1):
                for key in lexicon.keys(key[:-(i + 1)]):
                    return lexicon[key][0].decode('utf8')
        return self.guess_lemma(token, msd)

    def guess_lemma(self, token, msd):
        if len(token) < 3:
            return self.apply_rule(token, "(0,'',0,'')", msd)
        model = self.lemmatiser['model']
        if msd not in model:
            return token
        else:
            lemma = self.apply_rule(token, model[msd].predict(
                extract_features_lemma(token))[0], msd)
            if len(lemma) > 0:
                return lemma
            else:
                return token
    
    def apply_rule(self, token, rule, msd):
        rule = list(eval(rule))
        if msd:
            if msd[:2] == 'Np':
                lemma = token
            else:
                lemma = token.lower()
        else:
            lemma = token.lower()
        rule[2] = len(token) - rule[2]
        lemma = rule[1] + lemma[rule[0]:rule[2]] + rule[3]
        return lemma

    def test_me(self):
        t = Tokeniser()
        totag = t.only_tokens(t.tokenise(test_text))
        print(totag)
        tags = self.tag_lemmatise_sent(totag)
        print(tags)
    
    def test_custom(self, txt):
        t = Tokeniser()
        totag = t.only_tokens(t.tokenise(txt))
        tags = self.tag_lemmatise_sent(totag)
    
    def tag_and_lemmatise_tokens(self, tokens):
        return self.tag_lemmatise_sent(tokens)
    
    def lemmatise_token(self, token):
        return self.tag_lemmatise_sent([token])
